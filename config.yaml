training:
  n_envs: 4
  total_timesteps: 1000000
  batch_size: 2048
  n_epochs: 10
  learning_rate: 0.0003
  save_freq: 100000
  eval_freq: 50000

model:
  hidden_size: 512
  policy_layers: [512, 512, 256]
  value_layers: [512, 512, 256]
  activation: "relu"
  dropout: 0.1
  use_layer_norm: true
  use_lr_schedule: true

environment:
  max_steps: 300
  timeout: 300
  team_size: 1
  difficulty: "medium"
  spawn_opponents: false  # Faster training without opponents
  obs_builder: "AdvancedObs"
  action_parser: "DiscreteAction"
  reward_function: "CombinedReward"
  gravity: -9.8
  ball_drag: 0.03
  car_drag: 0.03
  boost_force: 20.0
  jump_force: 5.0
  flip_force: 7.0

paths:
  log_dir: "logs"
  model_dir: "models"
  tensorboard_dir: "tensorboard_logs"
  config_dir: "configs"

evaluation:
  n_eval_episodes: 10
  eval_deterministic: true
  render_eval: false

logging:
  log_level: "INFO"
  save_replay_buffer: true
  save_vecnormalize: true
